{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "\n",
    "**Probelm Statement:** Make a classifier which takes in a job description and gives the department name for it.\n",
    "*   Use a neural network model\n",
    "*   Make use of a pre-trained Word Embeddings (example: Word2Vec, GloVe, etc.)\n",
    "*   Calculate the accuracy on a test set (data not used to train the model)\n",
    "\n",
    "**Problem Solving Approach:** \n",
    "_Provide a brief description of steps you followed for solving this problem_\n",
    "1. Create a Dataframe by taking the required columns from the given data.\n",
    "2. Create word to vec matrix by preformng various operation like tokenizing, stemming etc. using nltk.\n",
    "3. Remove the rows having null values.\n",
    "4. Split the prepared data into train, validation and test set.\n",
    "5. Train the model using train and validation data.\n",
    "6. Finally, test the result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Text Preprocessing\n",
    "\n",
    "_Include all text preprocesing steps like processing of json,csv files & data cleaning in this part._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccessary packages in below cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to filter the words\n",
    "def get_filter_words(example_sent):\n",
    "  \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    example_sent=example_sent.lower()\n",
    "    word_tokens = word_tokenize(example_sent)\n",
    "    \n",
    "    filtered_words = [] \n",
    "    for i in word_tokens:\n",
    "        if i.isalpha():\n",
    "            filtered_words.append(i)\n",
    "            \n",
    "     \n",
    "    filtered_words1 = []\n",
    "    for w in filtered_words: \n",
    "        if w not in stop_words: \n",
    "            filtered_words1.append(w) \n",
    "      \n",
    "    filtered_words1=set(filtered_words1)\n",
    "    \n",
    "    ps=PorterStemmer()\n",
    "    \n",
    "    temp=[]\n",
    "    for i in filtered_words1:\n",
    "        temp.append(ps.stem(i))\n",
    "        \n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depart_doc=pd.read_csv('data/document_departments.csv')\n",
    "\n",
    "li=[]\n",
    "m=depart_doc[\"Document ID\"]\n",
    "\n",
    "for i in range(len(m)):\n",
    "    #print(i)\n",
    "    st='data/docs/'+str(m[i])+\".json\"\n",
    "    li.append([depart_doc[\"Department\"][i]])\n",
    "    \n",
    "    with open(st) as f:\n",
    "        dat = json.load(f)\n",
    "        des=dat[\"jd_information\"][\"description\"]\n",
    "        if des==\"\":\n",
    "            des=pd.NaT\n",
    "        li[i].append(des)\n",
    "    \n",
    "    \n",
    "data=pd.DataFrame(columns=[\"ID\",\"Description\"],data=li)\n",
    "\n",
    "data.count()\n",
    "\n",
    "dataset=data.dropna()\n",
    "\n",
    "dataset.groupby(\"ID\").count()\n",
    "\n",
    "columns=set([])\n",
    "rows=[]\n",
    "dictn={}\n",
    "\n",
    "for i in dataset[\"Description\"]:\n",
    "    t=get_filter_words(i)\n",
    "    for wrd in t:\n",
    "        columns.add(wrd)\n",
    "    rows.append(t)\n",
    "    \n",
    "for i in columns:\n",
    "    dictn[i]=[]\n",
    "    for j in rows:\n",
    "        dictn[i].append(j.count(i))\n",
    "        \n",
    "        \n",
    "df=pd.DataFrame(data=dictn)\n",
    "df[\"Category\"],_=pd.factorize(dataset[\"ID\"])\n",
    "\n",
    "df.to_csv(path_or_buf=\"spot_mod_data.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Exploratoty Data Analysis\n",
    "\n",
    "_Include EDA steps like finding distribution of Departments in this part, you may also use plots for EDA._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby([\"Category\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III: Modelling & Evaluation\n",
    "\n",
    "_Include all model prepration & evaluation steps in this part._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_csv('data/spot_mod_data.csv')\n",
    "\n",
    "# drop the columns having count<4\n",
    "q1=dataset.iloc[:,:-1].sum(axis=0)\n",
    "q2=list(q1[q1<4].index)\n",
    "ds=dataset.drop(columns=q2)\n",
    "\n",
    "# save the data into csv to get trin, validate, test files\n",
    "i=int(0.2* len(ds))\n",
    "j=int(0.2*i)\n",
    "ds[j:-i].to_csv(path_or_buf=\"train.csv\",index=False)\n",
    "ds[:j].to_csv(path_or_buf=\"validate.csv\",index=False)\n",
    "ds[-i:].to_csv(path_or_buf=\"test.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as data\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "\n",
    "#create a class to get the tensor data from csv\n",
    "class my_points(data.Dataset):\n",
    "    def __init__(self, filename):\n",
    "        pd_data = pd.read_csv(filename).values\n",
    "        self.data = pd_data[:,:-1] \n",
    "        self.target = pd_data[:,-1:] \n",
    "        self.n_samples = self.data.shape[0]\n",
    "    \n",
    "    def __len__(self):   # Length of the dataset.\n",
    "        return self.n_samples\n",
    "    \n",
    "    def __getitem__(self, index):   # Function that returns one point and one label.\n",
    "        return torch.Tensor(self.data[index]), torch.Tensor(self.target[index])\n",
    "    \n",
    "# create the dataloader.\n",
    "train= my_points('train.csv')\n",
    "test=my_points('test.csv')\n",
    "validate=my_points('validate.csv')\n",
    "\n",
    "batch_size = 20\n",
    "trainloader = data.DataLoader(train,batch_size=batch_size,num_workers=0)\n",
    "testloader = data.DataLoader(test,batch_size=batch_size,num_workers=0)\n",
    "validloader = data.DataLoader(validate,batch_size=batch_size,num_workers=0)\n",
    "\n",
    "\n",
    "# define a model\n",
    "model = nn.Sequential(nn.Linear(1203, 512),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(512, 256),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, 27),\n",
    "                      nn.LogSoftmax(dim=1)\n",
    "                      )\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "\n",
    "n_epochs=1000\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    # train the model\n",
    "    model.train() # prep model for training\n",
    "    for datas, target in trainloader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(datas)\n",
    "        target = target.long()\n",
    "        target=target.squeeze(1)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*datas.size(0)\n",
    "        \n",
    "       \n",
    "    # validate the model \n",
    "    model.eval() # prep model for evaluation\n",
    "    for data1, target in validloader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data1)\n",
    "        # calculate the loss\n",
    "        target = target.long()\n",
    "        target=target.squeeze(1)\n",
    "        loss = criterion(output, target)\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data1.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(trainloader.dataset)\n",
    "    valid_loss = valid_loss/len(validloader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model, 's_model.pt')\n",
    "        valid_loss_min = valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "model=torch.load('s_model.pt')\n",
    "  \n",
    "# Test the model\n",
    "\n",
    "test_loss=0.0\n",
    "\n",
    "matched=[0 for i in range(27)]\n",
    "total=[0 for i in range(27)]\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for dat,target in testloader:\n",
    "        \n",
    "        output=model(dat)\n",
    "        #print(output)\n",
    "        _,pred=torch.max(output,1)\n",
    "        target = target.long()\n",
    "        target=target.squeeze(1)\n",
    "        loss=criterion(output,target)\n",
    "        test_loss+=loss.item()*dat.size(0)\n",
    "        \n",
    "        for i in range(len(pred)):\n",
    "            xx=int(target[i].item())\n",
    "            if xx==pred[i]:\n",
    "                matched[xx]+=1\n",
    "            total[xx]+=1\n",
    "            \n",
    "    test_loss=test_loss/len(test)\n",
    "\n",
    "print(\"Test Loss\",test_loss)\n",
    "    \n",
    "print(\"Overall Accuracy:\",(sum(matched)/sum(total))*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Summary:**\n",
    "_Provide a brief summary of results obtained like model accuracy & other insights based on EDA & your interpretations_\n",
    "\n",
    "1. There are more than 400 rows having null value, i.e no description present\n",
    "2. Overall Accuracy is approx 65% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
